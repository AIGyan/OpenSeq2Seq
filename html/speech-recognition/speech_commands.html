

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Speech Commands &mdash; OpenSeq2Seq 0.2 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/theme_override.css" type="text/css" />
  <link rel="stylesheet" href="../_static/theme_override.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Creation of Synthetic Data" href="synthetic_dataset.html" />
    <link rel="prev" title="Jasper" href="jasper.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> OpenSeq2Seq
          

          
            
            <img src="../_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation Instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../machine-translation.html">Machine Translation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../speech-recognition.html">Speech Recognition</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../speech-recognition.html#models">Models</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="deepspeech2.html">DeepSpeech2</a></li>
<li class="toctree-l3"><a class="reference internal" href="wave2letter.html">Wave2Letter+</a></li>
<li class="toctree-l3"><a class="reference internal" href="jasper.html">Jasper</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Speech Commands</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dataset">Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-details-and-results">Model Details and Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="#mixed-precision">Mixed Precision</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../speech-recognition.html#getting-started">Getting started</a></li>
<li class="toctree-l2"><a class="reference internal" href="../speech-recognition.html#synthetic-data">Synthetic data</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../speech-synthesis.html">Speech Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../language-model.html">Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sentiment-analysis.html">Sentiment Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distr-training.html">Multi-GPU and Distributed Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mixed-precision.html">Mixed Precision Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../in-depth-tutorials.html">In-depth Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../interactive-infer-demos.html">Interactive Infer Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-docs/modules.html">API documentation</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">OpenSeq2Seq</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../speech-recognition.html">Speech Recognition</a> &raquo;</li>
        
      <li>Speech Commands</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/speech-recognition/speech_commands.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="speech-commands">
<span id="id1"></span><h1>Speech Commands<a class="headerlink" href="#speech-commands" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>The ability to recognize spoken commands with high accuracy can be useful in a variety of contexts. To this end, Google recently released the Speech Commands dataset (see <a class="reference external" href="https://arxiv.org/abs/1804.03209">paper</a>), which contains short audio clips of a fixed number of command words such as “stop”, “go”, “up”, “down”, etc spoken by a large number of speakers. To promote the use of the set, Google also hosted a <a class="reference external" href="https://www.kaggle.com/c/tensorflow-speech-recognition-challenge">Kaggle competition</a>, in which the winning team attained a multiclass accuracy of 91%.</p>
<p>We started by experimenting with applying OpenSeq2Seq’s existing image classification models on mel spectrograms of the audio clips and found that they worked surprisingly well. Adding data augmentation further improved the results.</p>
</div>
<div class="section" id="dataset">
<h2>Dataset<a class="headerlink" href="#dataset" title="Permalink to this headline">¶</a></h2>
<p>Google released two versions of the dataset with the first version containing 65k samples over 30 classes and the second containing 110k samples over 35 classes. However, the Kaggle contest specification used only 10 of the provided classes, grouped the others as “unknown” and added “silence” for a total of 12 labels. We refer to these datasets as v1-12, v1-30 and v2, and have separate metrics for each version in order to compare to the different metrics used by other papers.</p>
<p>To preprocess a given version, we run <code class="docutils literal notranslate"><span class="pre">speech_commands_preprocessing.py</span></code> which balances the number of samples in each class by duplicating the samples in each class. This effectively grows the dataset, as we apply random transformations to each sample in the data layer. The script then separates each class into training, validation and test samples via an 80-10-10 split.</p>
<p>These samples are fed into the data layer, which randomly stretches and adds noise to each audio sample. These augmented samples are then converted into mel spectrograms and either randomly sliced or symmetrically padded with zeros until they are fixed dimension and square (120x120). This dataset is then cached during the first epoch in order to increase GPU utilization.</p>
</div>
<div class="section" id="model-details-and-results">
<h2>Model Details and Results<a class="headerlink" href="#model-details-and-results" title="Permalink to this headline">¶</a></h2>
<p>The output of the data layer is a 120x120 image, which is fed into an unmodified ResNet-50 architecture consisting of several convolutional blocks. We obtained the following results by adding data augmentation and training ResNet-50 in float over 10 epochs.</p>
<table border="1" class="colwidths-given docutils">
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Dataset</th>
<th class="head">Validation Accuracy</th>
<th class="head">Test Accuracy</th>
<th class="head">Checkpoint</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>v1-12</td>
<td>94.7%</td>
<td>94.7%</td>
<td><a class="reference external" href="https://drive.google.com/open?id=1NDPaUuwuL2G2ZhgBL75RHOfpIs4ewCOg">link</a></td>
</tr>
<tr class="row-odd"><td>v1-30</td>
<td>96.6%</td>
<td>95.8%</td>
<td><a class="reference external" href="https://drive.google.com/open?id=1MwUX8EqGEjrSbxOyGHOLeOeqco-RF9vT">link</a></td>
</tr>
<tr class="row-even"><td>v2</td>
<td>95.0%</td>
<td>95.1%</td>
<td><a class="reference external" href="https://drive.google.com/open?id=199HYZRX2O1tWFGZYkYP_E-R8SvrHIiqn">link</a></td>
</tr>
</tbody>
</table>
<p>The configuration file used for all models is <a class="reference external" href="https://github.com/NVIDIA/OpenSeq2Seq/blob/master/example_configs/image2label/speech_commands_float.py">here</a>. The only change between datasets is the <code class="docutils literal notranslate"><span class="pre">dataset_version</span></code> parameter, which should be set to one of <code class="docutils literal notranslate"><span class="pre">v1-12</span></code>, <code class="docutils literal notranslate"><span class="pre">v1-30</span></code> or <code class="docutils literal notranslate"><span class="pre">v2</span></code>.</p>
</div>
<div class="section" id="mixed-precision">
<h2>Mixed Precision<a class="headerlink" href="#mixed-precision" title="Permalink to this headline">¶</a></h2>
<p>We found that the model trains just as well in mixed precision using a constant loss scaling of 512. This roughly halves the required GPU memory.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="synthetic_dataset.html" class="btn btn-neutral float-right" title="Creation of Synthetic Data" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="jasper.html" class="btn btn-neutral" title="Jasper" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, NVIDIA.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'0.2',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          
          SphinxRtdTheme.Navigation.enableSticky();
          
      });
  </script>  
  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #64d81c;
    }
    .wy-side-nav-search > div.version {
      color: #ffffff;
    }
    .wy-side-nav-search > img {
      max-width: 150px;
    }
    .wy-side-nav-search > a {
      font-size: 23px;
    }
  </style>


</body>
</html>